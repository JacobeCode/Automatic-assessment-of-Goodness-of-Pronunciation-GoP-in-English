# This file was created by the command:
# steps/nnet3/xconfig_to_configs.py --xconfig-file=exp/rnnlm_lstm_1a/config/xconfig --config-dir=exp/rnnlm_lstm_1a/config/nnet
# It contains the entire neural network, but with those
# components that would normally require fixed vectors/matrices
# read from disk, replaced with random initialization
# (this applies to the LDA-like transform and the
# presoftmax-prior-scale, if applicable).  This file
# is used only to work out the left-context and right-context
# of the network.

input-node name=input dim=1024
component name=tdnn1.affine type=NaturalGradientAffineComponent input-dim=2048 output-dim=1024  max-change=0.75
component-node name=tdnn1.affine component=tdnn1.affine input=Append(input, IfDefined(Offset(input, -1)))
component name=tdnn1.relu type=RectifiedLinearComponent dim=1024 self-repair-scale=1e-05
component-node name=tdnn1.relu component=tdnn1.relu input=tdnn1.affine
component name=tdnn1.renorm type=NormalizeComponent dim=1024 target-rms=1.0 add-log-stddev=false
component-node name=tdnn1.renorm component=tdnn1.renorm input=tdnn1.relu
##  Begin LTSM layer 'lstm1'
# Gate control: contains W_i, W_f, W_c and W_o matrices as blocks.
component name=lstm1.W_all type=NaturalGradientAffineComponent input-dim=1280 output-dim=4096  max-change=1.5 
# The core LSTM nonlinearity, implemented as a single component.
# Input = (i_part, f_part, c_part, o_part, c_{t-1}), output = (c_t, m_t)
# See cu-math.h:ComputeLstmNonlinearity() for details.
component name=lstm1.lstm_nonlin type=LstmNonlinearityComponent cell-dim=1024 use-dropout=false  max-change=0.75 
# Component for backprop truncation, to avoid gradient blowup in long training examples.
component name=lstm1.cr_trunc type=BackpropTruncationComponent dim=1280 clipping-threshold=30.0 zeroing-threshold=15.0 zeroing-interval=20 recurrence-interval=1 scale=1.0
# Component specific to 'projected' LSTM (LSTMP), contains both recurrent
# and non-recurrent projections
component name=lstm1.W_rp type=NaturalGradientAffineComponent input-dim=1024 output-dim=512  max-change=1.5 
###  Nodes for the components above.
component-node name=lstm1.W_all component=lstm1.W_all input=Append(tdnn1.renorm, IfDefined(Offset(lstm1.r_trunc, -1)))
component-node name=lstm1.lstm_nonlin component=lstm1.lstm_nonlin input=Append(lstm1.W_all, IfDefined(Offset(lstm1.c_trunc, -1)))
dim-range-node name=lstm1.c input-node=lstm1.lstm_nonlin dim-offset=0 dim=1024
dim-range-node name=lstm1.m input-node=lstm1.lstm_nonlin dim-offset=1024 dim=1024
# lstm1.rp is the output node of this layer (if we're not including batchnorm)
component-node name=lstm1.rp component=lstm1.W_rp input=lstm1.m
dim-range-node name=lstm1.r input-node=lstm1.rp dim-offset=0 dim=256
# Note: it's not 100% efficient that we have to stitch the c
# and r back together to truncate them but it probably
# makes the deriv truncation more accurate .
component-node name=lstm1.cr_trunc component=lstm1.cr_trunc input=Append(lstm1.c, lstm1.r)
dim-range-node name=lstm1.c_trunc input-node=lstm1.cr_trunc dim-offset=0 dim=1024
dim-range-node name=lstm1.r_trunc input-node=lstm1.cr_trunc dim-offset=1024 dim=256
### End LSTM Layer 'lstm1'
component name=tdnn2.affine type=NaturalGradientAffineComponent input-dim=1024 output-dim=1024  max-change=0.75
component-node name=tdnn2.affine component=tdnn2.affine input=Append(lstm1.rp, IfDefined(Offset(lstm1.rp, -3)))
component name=tdnn2.relu type=RectifiedLinearComponent dim=1024 self-repair-scale=1e-05
component-node name=tdnn2.relu component=tdnn2.relu input=tdnn2.affine
component name=tdnn2.renorm type=NormalizeComponent dim=1024 target-rms=1.0 add-log-stddev=false
component-node name=tdnn2.renorm component=tdnn2.renorm input=tdnn2.relu
##  Begin LTSM layer 'lstm2'
# Gate control: contains W_i, W_f, W_c and W_o matrices as blocks.
component name=lstm2.W_all type=NaturalGradientAffineComponent input-dim=1280 output-dim=4096  max-change=1.5 
# The core LSTM nonlinearity, implemented as a single component.
# Input = (i_part, f_part, c_part, o_part, c_{t-1}), output = (c_t, m_t)
# See cu-math.h:ComputeLstmNonlinearity() for details.
component name=lstm2.lstm_nonlin type=LstmNonlinearityComponent cell-dim=1024 use-dropout=false  max-change=0.75 
# Component for backprop truncation, to avoid gradient blowup in long training examples.
component name=lstm2.cr_trunc type=BackpropTruncationComponent dim=1280 clipping-threshold=30.0 zeroing-threshold=15.0 zeroing-interval=20 recurrence-interval=1 scale=1.0
# Component specific to 'projected' LSTM (LSTMP), contains both recurrent
# and non-recurrent projections
component name=lstm2.W_rp type=NaturalGradientAffineComponent input-dim=1024 output-dim=512  max-change=1.5 
###  Nodes for the components above.
component-node name=lstm2.W_all component=lstm2.W_all input=Append(tdnn2.renorm, IfDefined(Offset(lstm2.r_trunc, -1)))
component-node name=lstm2.lstm_nonlin component=lstm2.lstm_nonlin input=Append(lstm2.W_all, IfDefined(Offset(lstm2.c_trunc, -1)))
dim-range-node name=lstm2.c input-node=lstm2.lstm_nonlin dim-offset=0 dim=1024
dim-range-node name=lstm2.m input-node=lstm2.lstm_nonlin dim-offset=1024 dim=1024
# lstm2.rp is the output node of this layer (if we're not including batchnorm)
component-node name=lstm2.rp component=lstm2.W_rp input=lstm2.m
dim-range-node name=lstm2.r input-node=lstm2.rp dim-offset=0 dim=256
# Note: it's not 100% efficient that we have to stitch the c
# and r back together to truncate them but it probably
# makes the deriv truncation more accurate .
component-node name=lstm2.cr_trunc component=lstm2.cr_trunc input=Append(lstm2.c, lstm2.r)
dim-range-node name=lstm2.c_trunc input-node=lstm2.cr_trunc dim-offset=0 dim=1024
dim-range-node name=lstm2.r_trunc input-node=lstm2.cr_trunc dim-offset=1024 dim=256
### End LSTM Layer 'lstm2'
component name=tdnn3.affine type=NaturalGradientAffineComponent input-dim=1024 output-dim=1024  max-change=0.75
component-node name=tdnn3.affine component=tdnn3.affine input=Append(lstm2.rp, IfDefined(Offset(lstm2.rp, -3)))
component name=tdnn3.relu type=RectifiedLinearComponent dim=1024 self-repair-scale=1e-05
component-node name=tdnn3.relu component=tdnn3.relu input=tdnn3.affine
component name=tdnn3.renorm type=NormalizeComponent dim=1024 target-rms=1.0 add-log-stddev=false
component-node name=tdnn3.renorm component=tdnn3.renorm input=tdnn3.relu
component name=output.affine type=NaturalGradientAffineComponent input-dim=1024 output-dim=1024  max-change=1.5 param-stddev=0.0 bias-stddev=0.0
component-node name=output.affine component=output.affine input=tdnn3.renorm
output-node name=output input=output.affine objective=linear
